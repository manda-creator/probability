{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XXDeo-aGOAXF"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9XRGdjHNOE9D"
      },
      "outputs": [],
      "source": [
        "#@title ##### Licensed under the Apache License, Version 2.0 (the \"License\"); { display-mode: \"form\" }\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KJihamFwOLUT"
      },
      "source": [
        "# Sequential Monte Carlo (Particle filter) in TFP\n",
        "\n",
        "\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/python/experimental/mcmc/examples/smc_demo.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003eRun in Google Colab\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "  \u003ctd\u003e\n",
        "    \u003ca target=\"_blank\" href=\"https://github.com/tensorflow/probability/blob/master/tensorflow_probability/python/experimental/mcmc/examples/smc_demo.ipynb\"\u003e\u003cimg src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003eView source on GitHub\u003c/a\u003e\n",
        "  \u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uiR4-VOt9NFX"
      },
      "source": [
        "### Dependencies \u0026 Prerequisites\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0JGqyfetR1jD"
      },
      "outputs": [],
      "source": [
        "# !pip3 install -q git+git://github.com/arviz-devs/arviz.git\n",
        "# !pip3 install -q tf-nightly-gpu tfp-nightly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "coUnDhkpT5_6"
      },
      "outputs": [],
      "source": [
        "#@title Imports and setups\n",
        "dark_mode = False #@param {type:\"boolean\"}\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import arviz as az\n",
        "\n",
        "import tensorflow.compat.v1 as tf1\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow_probability.python.internal import prefer_static as ps\n",
        "from tensorflow_probability.python.mcmc.internal import util as mcmc_util\n",
        "from tensorflow_probability.python.mcmc.transformed_kernel import (\n",
        "    make_transform_fn, make_transformed_log_prob)\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "\n",
        "tf.enable_v2_behavior()\n",
        "\n",
        "sns.set(style=\"ticks\", context=\"talk\")\n",
        "if dark_mode:\n",
        "  plt.style.use(\"dark_background\")\n",
        "  dot_color = 'w'\n",
        "else:\n",
        "  plt.style.use(\"seaborn-darkgrid\")\n",
        "  dot_color = 'k'\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4j4Q5mBtZSiw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python version\")\n",
        "print(sys.version)\n",
        "print(\"Version info.\")\n",
        "print(sys.version_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WYEnkKtRNoWO"
      },
      "outputs": [],
      "source": [
        "with tf1.Session() as session:\n",
        "  pprint(session.list_devices())\n",
        "\n",
        "if tf.test.gpu_device_name() != '/device:GPU:0':\n",
        "  USE_XLA = False\n",
        "else:\n",
        "  USE_XLA = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "T6lzcA0-ZlAv"
      },
      "outputs": [],
      "source": [
        "print(\"Eager mode: {}\".format(tf.executing_eagerly()))\n",
        "print(\"XLA: {}\".format(USE_XLA))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5MXhoA99-FT2"
      },
      "source": [
        "# An introduction to Sequential Monte Carlo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "veXfBb2C-OBp"
      },
      "source": [
        "Particle filters[1], or Sequential Monte Carlo methods[2] are a set of very flexible simulation-based methods that use a set of particles (also called samples) to represent the posterior distribution of some stochastic process. Different from Markov Chain Monte Carlo method that also uses samples to represent the posterior distribution, SMC usually mutates and updates on the same set of samples.\n",
        "\n",
        "\n",
        "![alt text](https://d3i71xaburhd42.cloudfront.net/006e8089dad4183deeeda2e1f5038d7a3663e614/3-Figure1-1.png)  \n",
        "\n",
        "_figure from **Two Stage Particle Filter for Nonlinear Bayesian Estimation** (DOI:10.1109/ACCESS.2018.2808922)_\n",
        "\n",
        "*   [1] https://en.wikipedia.org/wiki/Particle_filter\n",
        "*   [2] Doucet, A., De Freitas, N., \u0026 Gordon, N. (2001). [An introduction to sequential Monte Carlo methods](https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf). In _Sequential Monte Carlo methods  in practice_ (pp. 3-14). Springer, New York, NY.\n",
        "*   [3] https://en.wikipedia.org/wiki/Approximate_Bayesian_computation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Hc1UMuJcvj7"
      },
      "source": [
        "## Simple example from Doucet, A., De Freitas, N., \u0026 Gordon, N. (2001)\n",
        "\n",
        "A demonstration of Bootstrap filter (AKA Sequential importance resampling).\n",
        "\n",
        "For more details see http://mlg.eng.cam.ac.uk/thang/docs/talks/rcc_smc.pdf and https://www.stats.ox.ac.uk/~doucet/doucet_defreitas_gordon_smcbookintro.pdf\n",
        "\n",
        "The model:\n",
        "$$\n",
        "x_t = \\frac{1}{2}x_{t-1} + 25\\frac{x_{t-1}}{1+x^2_{t-1}} + 8 \\text{cos}(1.2t) + v_t\n",
        "$$\n",
        "$$\n",
        "y_t = \\frac{x^2_t}{20} + w_t\n",
        "$$\n",
        "\n",
        "where $x_1 \\sim N(0, \\sigma^2_1)$, $v_t$ and $w_t$ are mutually independent white Gaussian noises, $v_k \\sim N(0, \\sigma^2_v)$ and $w_k \\sim N(0, \\sigma^2_w)$ with $\\sigma^2_1 = 10$, $\\sigma^2_v = 10$, $\\sigma^2_w = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Am2Rs7cyc2eH"
      },
      "outputs": [],
      "source": [
        "sigma_1 = 10.\n",
        "sigma_v = 10.\n",
        "sigma_w = 1.\n",
        "\n",
        "def x_next_step(x_previous_step, time):\n",
        "  return (.5 * x_previous_step \n",
        "          + 25 * x_previous_step / (1 + x_previous_step**2)\n",
        "          + 8 * tf.cos(1.2 * time)\n",
        "          + tf.random.normal(ps.shape(x_previous_step), stddev=sigma_v))\n",
        "\n",
        "def y_current_step(x_current_step):\n",
        "  return tf.random.normal(ps.shape(x_current_step), \n",
        "                          mean=x_current_step ** 2 / 20., \n",
        "                          stddev=sigma_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ywvbmtS9jpO0"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
        "N = 500\n",
        "x_current = tfd.Normal(0., sigma_1).sample(N)\n",
        "y_current = y_current_step(x_current)\n",
        "x_timestep = []\n",
        "index = np.random.randint(N)\n",
        "for i in range(101):\n",
        "  x_timestep.append(x_current[index].numpy())\n",
        "  x_current = x_next_step(x_current, i)\n",
        "  y_current = y_current_step(x_current)\n",
        "  if i % 10 == 0:\n",
        "    ax.plot(i+np.random.randn(N)*.5,\n",
        "            x_current,\n",
        "            'o', color=dot_color, alpha=.05);\n",
        "\n",
        "ax.plot(x_timestep, lw=2, label='One realization')\n",
        "ax.set_title('Distribution of State (x)')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sLSAeWsImOoD"
      },
      "outputs": [],
      "source": [
        "SCALE = 10.\n",
        "# For sampling $\\tilde{x_t} \\sim P(x_t|x_{t-1})$\n",
        "GaussianKernel = lambda x: x + tf.random.normal(\n",
        "    ps.shape(x), stddev=SCALE)\n",
        "\n",
        "def gen_log_prob_fn(states):\n",
        "  return tfd.Normal(states**2 / 20., sigma_w).log_prob\n",
        "\n",
        "def important_sample_and_select(previous_states, current_observed):\n",
        "  state_tilde = GaussianKernel(previous_states)\n",
        "  importance_weight = gen_log_prob_fn(state_tilde)(current_observed)\n",
        "  resample_index = tf.random.categorical(importance_weight[tf.newaxis, ...],\n",
        "                                         tf.reduce_prod(*ps.shape(previous_states)))\n",
        "  return tf.reshape(tf.gather(state_tilde, resample_index),\n",
        "                    ps.shape(previous_states))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tl8eq34H1Jgd"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
        "\n",
        "x_current = tfd.Normal(0., sigma_1).sample()\n",
        "y_current = y_current_step(x_current)\n",
        "\n",
        "n_particle = 1000\n",
        "# Initialize particles by sampling from the prior\n",
        "x_state = tfd.Normal(0., sigma_1).sample(n_particle)\n",
        "x_timestep = []\n",
        "x_state_time = []\n",
        "\n",
        "for i in range(1, 101):\n",
        "  x_timestep.append(x_current.numpy())\n",
        "  x_current = x_next_step(x_current, i)\n",
        "  y_current = y_current_step(x_current)\n",
        "  x_state = important_sample_and_select(x_state, y_current)\n",
        "  if i % 10 == 1:\n",
        "    x_state_time.append((i, x_state.numpy()))\n",
        "    ax.plot(i+np.random.randn(n_particle),\n",
        "            x_state,\n",
        "            'o', color=dot_color, alpha=.05);\n",
        "    ax.plot(i, x_current, 'o', color='r');\n",
        "\n",
        "ax.plot(x_timestep, alpha=.5, label='Latent state (unobserved)')\n",
        "ax.set_title('Posterior of State (x)')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xWTlnO6tewlP"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "for t, samples in x_state_time:\n",
        "  hist, edge = np.histogram(samples, 50, density=True)\n",
        "  ax.plot((edge[:-1] + edge[1:])/2, hist, zs=t, zdir='y', color='k', alpha=.75)\n",
        "\n",
        "ax.set_xlabel('X_t')\n",
        "ax.set_ylabel('Time (t)')\n",
        "ax.set_zlabel('p(x_t|y_{1:t})')\n",
        "\n",
        "ax.view_init(elev=40., azim=-35)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "brr5B9rFdFmh"
      },
      "source": [
        "# SMC driver\n",
        "\n",
        "SMC works by moving through successive stages. At each stage the inverse temperature $\\beta$ is increased a little bit (starting from 0 up to 1).  \n",
        "When $\\beta = 0$ we have the prior distribution and when $\\beta = 1$ we have the posterior distribution.  \n",
        "\n",
        "So in more general terms we are always computing samples from a tempered posterior that we can write as:  \n",
        "$$\n",
        "        p(\\theta \\mid y)_{\\beta} \\propto p(y \\mid \\theta)^{\\beta} p(\\theta)\n",
        "$$\n",
        "\n",
        "[A summary of the algorithm](https://github.com/pymc-devs/pymc3/blob/master/pymc3/smc/sample_smc.py) is:\n",
        "- Initialization:\n",
        "  1. Initialize $\\beta$ at zero and stage at zero.\n",
        "  2. Generate N samples $S_{\\beta}$ from the prior (because when $\\beta = 0$ the tempered posterior is the prior).\n",
        "- Sampling: Repeat from until $\\beta \\ge 1$(implemented as a `tf.while_loop`)\n",
        "  3. Increase $\\beta$ in order to make the effective sample size equals some predefined value (we use $N_t$, where $t$ is 0.5 by default).\n",
        "  4. Compute a set of N importance weights W. The weights are computed as the ratio of the likelihoods of a sample at stage `i+1` and stage `i`.\n",
        "  5. Obtain $S_{w}$ by re-sampling according to W.\n",
        "  6. Use W to compute the covariance for the proposal distribution.\n",
        "  7. For stages other than 0 use the acceptance rate from the previous stage to estimate the scaling of the proposal distribution and `n_steps`.\n",
        "  8. Run N Metropolis chains (each one of length `n_steps`), starting each one from a different sample in $S_{w}$.\n",
        "- Collect result\n",
        "  9. The final result is a collection of N samples from the posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7rqYMO8t4CY0"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.experimental.mcmc.sample_sequential_monte_carlo import make_rwmh_kernel_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V1Bt1UWe0WFO"
      },
      "outputs": [],
      "source": [
        "sample_sequential_monte_carlo_chain = tfp.experimental.mcmc.sample_sequential_monte_carlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "5L6hySJy-vyy"
      },
      "outputs": [],
      "source": [
        "tfp.experimental.mcmc.sample_sequential_monte_carlo.__globals__['PRINT_DEBUG'] = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pVtZfWso2fGG"
      },
      "source": [
        "As demonstration, we are setting up a MvNormal Mixture likelihood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-bKZiXcdCBIS"
      },
      "outputs": [],
      "source": [
        "n = 4\n",
        "mu = np.ones(n) * (1. / 2)\n",
        "w = 0.1\n",
        "\n",
        "prior_dist = tfd.Sample(tfd.Normal(0., 10.), sample_shape=n)\n",
        "likelihood_dist = tfd.MixtureSameFamily(\n",
        "    mixture_distribution=tfd.Categorical(\n",
        "        probs=[w, 1.-w]),\n",
        "    components_distribution=tfd.MultivariateNormalDiag(\n",
        "        loc=np.asarray([mu, -mu]).astype(np.float32),\n",
        "        scale_identity_multiplier=[.1, .2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "h7PP_HnPEA1g"
      },
      "outputs": [],
      "source": [
        "@tf.function()\n",
        "def run_smc():\n",
        "  n_stage, final_state, final_kernel_results = sample_sequential_monte_carlo_chain(\n",
        "    prior_dist.log_prob,\n",
        "    likelihood_dist.log_prob,\n",
        "    prior_dist.sample(2000),\n",
        "    make_kernel_fn=make_rwmh_kernel_fn,\n",
        "    max_num_steps=50,\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results\n",
        "\n",
        "n_stage, final_state, final_kernel_results = run_smc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1wgJeRtREA14"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 3))\n",
        "plt.hist(np.ravel(final_state),\n",
        "         bins=100, density=True, histtype='step', lw=2, label='SMC')\n",
        "plt.hist(np.ravel(likelihood_dist.sample(2000)),\n",
        "         bins=100, density=True, histtype='step', lw=2, label='Latent Truth')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d5PvSW_FOp6b"
      },
      "source": [
        "To do Approximate Bayesian Computation, you can implemented a Pseudo Likelihood function and pass it as arg `likelihood_log_prob_fn`. For example, you can write a function that takes the particles as input, simulate pseudo observation, and compute the distance between the simulated observation and the true observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GX8s7kIDdKM7"
      },
      "source": [
        "# Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cmIDM8oSA2ci"
      },
      "source": [
        "## Robust regression with mixture likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "4rT8Ug8NA93h"
      },
      "outputs": [],
      "source": [
        "#@title Set up data (Hogg 2010)\n",
        "# cut \u0026 pasted directly from the fetch_hogg2010test() function\n",
        "# identical to the original dataset as hardcoded in the Hogg 2010 paper\n",
        "\n",
        "dfhogg = pd.DataFrame(\n",
        "                np.array([[1, 201, 592, 61, 9, -0.84],\n",
        "                          [2, 244, 401, 25, 4, 0.31],\n",
        "                          [3, 47, 583, 38, 11, 0.64],\n",
        "                          [4, 287, 402, 15, 7, -0.27],\n",
        "                          [5, 203, 495, 21, 5, -0.33],\n",
        "                          [6, 58, 173, 15, 9, 0.67],\n",
        "                          [7, 210, 479, 27, 4, -0.02],\n",
        "                          [8, 202, 504, 14, 4, -0.05],\n",
        "                          [9, 198, 510, 30, 11, -0.84],\n",
        "                          [10, 158, 416, 16, 7, -0.69],\n",
        "                          [11, 165, 393, 14, 5, 0.30],\n",
        "                          [12, 201, 442, 25, 5, -0.46],\n",
        "                          [13, 157, 317, 52, 5, -0.03],\n",
        "                          [14, 131, 311, 16, 6, 0.50],\n",
        "                          [15, 166, 400, 34, 6, 0.73],\n",
        "                          [16, 160, 337, 31, 5, -0.52],\n",
        "                          [17, 186, 423, 42, 9, 0.90],\n",
        "                          [18, 125, 334, 26, 8, 0.40],\n",
        "                          [19, 218, 533, 16, 6, -0.78],\n",
        "                          [20, 146, 344, 22, 5, -0.56]]),\n",
        "                columns=['id','x','y','sigma_y','sigma_x','rho_xy'])\n",
        "\n",
        "dfhogg['id'] = dfhogg['id'].apply(lambda x: 'p{}'.format(int(x)))\n",
        "dfhogg.set_index('id', inplace=True)\n",
        "dfhogg.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2gp2WieqA93o"
      },
      "outputs": [],
      "source": [
        "dfhoggs = ((dfhogg[['x', 'y']] - dfhogg[['x', 'y']].mean(0)) / \n",
        "           (2 * dfhogg[['x', 'y']].std(0)))\n",
        "dfhoggs['sigma_x'] = dfhogg['sigma_x'] / ( 2 * dfhogg['x'].std())\n",
        "dfhoggs['sigma_y'] = dfhogg['sigma_y'] / ( 2 * dfhogg['y'].std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ks_oF6b6A93s"
      },
      "outputs": [],
      "source": [
        "dtype = tf.float64\n",
        "\n",
        "hyper_mean = tf.cast(0, dtype)\n",
        "hyper_scale = tf.cast(10, dtype)\n",
        "X = dfhoggs['x'].values\n",
        "sigma = dfhoggs['sigma_y'].values\n",
        "Y = dfhoggs['y'].values\n",
        "\n",
        "mdl_mixture = tfd.JointDistributionSequential([\n",
        "  tfd.Normal(loc=hyper_mean, scale=hyper_scale),\n",
        "  tfd.Normal(loc=hyper_mean, scale=hyper_scale),\n",
        "  tfd.Normal(loc=hyper_mean, scale=10.),\n",
        "  tfd.HalfNormal(scale=tf.cast(1., dtype)),\n",
        "  tfd.Uniform(low=tf.cast(0, dtype), high=.5),\n",
        "  lambda weight, sigma_out, mu_out, b1, b0: tfd.Independent(\n",
        "      tfd.Mixture(\n",
        "          tfd.Categorical(probs=tf.stack([\n",
        "            tf.repeat(1-weight[..., tf.newaxis], 20, axis=-1),\n",
        "            tf.repeat(weight[..., tf.newaxis], 20, axis=-1)\n",
        "            ], -1)),\n",
        "          [\n",
        "           tfd.Normal(loc=tf.squeeze(b0[..., tf.newaxis] + b1[..., tf.newaxis]*X),\n",
        "                      scale=np.squeeze(sigma)),\n",
        "           tfd.Normal(loc=mu_out[..., tf.newaxis], \n",
        "                      scale=tf.squeeze(sigma+sigma_out[..., tf.newaxis]))\n",
        "          ]\n",
        "    ), 1)\n",
        "], validate_args=True)\n",
        "\n",
        "mdl_mixture.log_prob(mdl_mixture.sample())\n",
        "mdl_mixture.log_prob(mdl_mixture.sample(7))\n",
        "mdl_mixture.resolve_graph()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iojcNBU-A93w"
      },
      "outputs": [],
      "source": [
        "# User provided, use samples from priors\n",
        "prior_jd = tfd.JointDistributionSequential([\n",
        "  tfd.Normal(loc=hyper_mean, scale=hyper_scale),\n",
        "  tfd.Normal(loc=hyper_mean, scale=hyper_scale),\n",
        "  tfd.Normal(loc=hyper_mean, scale=10.),\n",
        "  tfd.HalfNormal(scale=tf.cast(1., dtype)),           \n",
        "  tfd.Uniform(low=tf.cast(0, dtype), high=.5),\n",
        "], validate_args=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "pvAcdFqIA931"
      },
      "outputs": [],
      "source": [
        "draws = 5000\n",
        "init_population_ = prior_jd.sample(draws)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yFCzFB2eA934"
      },
      "outputs": [],
      "source": [
        "# bijector to map contrained parameters to real\n",
        "a, b = tf.constant(0., dtype), tf.constant(.5, dtype),\n",
        "\n",
        "# Interval transformation\n",
        "tfp_interval = tfb.Inline(\n",
        "    inverse_fn=(\n",
        "        lambda x: tf.math.log(x - a) - tf.math.log(b - x)),\n",
        "    forward_fn=(\n",
        "        lambda y: (b - a) * tf.sigmoid(y) + a),\n",
        "    forward_log_det_jacobian_fn=(\n",
        "        lambda x: tf.math.log(b - a) - 2. * tf.nn.softplus(-x) - x),\n",
        "    forward_min_event_ndims=0,\n",
        "    name=\"interval\")\n",
        "\n",
        "unconstraining_bijectors = [\n",
        "    tfb.Identity(),\n",
        "    tfb.Identity(),\n",
        "    tfb.Identity(),\n",
        "    tfb.Exp(),\n",
        "    tfp_interval,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MK-yWN88A937"
      },
      "outputs": [],
      "source": [
        "_inverse_transform = make_transform_fn(unconstraining_bijectors, 'inverse')\n",
        "_forward_transform = make_transform_fn(unconstraining_bijectors, 'forward')\n",
        "\n",
        "# Generate model prior_log_prob_fn and likelihood_log_prob_fn. User provided.\n",
        "prior_log_prob_fn_ = lambda *x: prior_jd.log_prob(x)\n",
        "prior_log_prob_fn = make_transformed_log_prob(\n",
        "    prior_log_prob_fn_,\n",
        "    unconstraining_bijectors,\n",
        "    direction='forward',\n",
        "    # TODO(b/72831017): Disable caching until gradient linkage\n",
        "    # generally works.\n",
        "    enable_bijector_caching=False)\n",
        "\n",
        "def likelihood_log_prob_fn_(b0, b1, mu_out, sigma_out, weight):\n",
        "  return tfd.Independent(\n",
        "    tfd.Mixture(\n",
        "        tfd.Categorical(probs=tf.stack([\n",
        "          tf.repeat(1-weight[..., tf.newaxis], 20, axis=-1),\n",
        "          tf.repeat(weight[..., tf.newaxis], 20, axis=-1)\n",
        "          ], -1)),\n",
        "        [\n",
        "         tfd.Normal(loc=b0[..., tf.newaxis] + b1[..., tf.newaxis]*X,\n",
        "                    scale=sigma),\n",
        "         tfd.Normal(loc=mu_out[..., tf.newaxis],\n",
        "                    scale=sigma+sigma_out[..., tf.newaxis])\n",
        "        ]\n",
        "  ), 1).log_prob(Y)\n",
        "\n",
        "def likelihood_log_prob_fn(*state_parts):\n",
        "  return likelihood_log_prob_fn_(*_forward_transform(state_parts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nhQU772CA93-"
      },
      "outputs": [],
      "source": [
        "# Evaluate the prior_log_prob_fn and likelihood_log_prob_fn on initial population\n",
        "prior_log_prob_val_ = prior_log_prob_fn_(*init_population_)\n",
        "\n",
        "init_population = _inverse_transform(init_population_)\n",
        "\n",
        "prior_log_prob_val = prior_log_prob_fn(*init_population)\n",
        "likelihood_log_prob_val = likelihood_log_prob_fn(*init_population)\n",
        "\n",
        "if True:  # Set True to debug\n",
        "  np.testing.assert_allclose(\n",
        "      prior_log_prob_val_ + likelihood_log_prob_val,\n",
        "      mdl_mixture.log_prob(*init_population_, Y))\n",
        "\n",
        "  model_logp = make_transformed_log_prob(\n",
        "      lambda *x: mdl_mixture.log_prob(*x, Y),\n",
        "      unconstraining_bijectors,\n",
        "      direction='forward',\n",
        "      # TODO(b/72831017): Disable caching until gradient linkage\n",
        "      # generally works.\n",
        "      enable_bijector_caching=False)\n",
        "\n",
        "  np.testing.assert_allclose(\n",
        "      prior_log_prob_val + likelihood_log_prob_val,\n",
        "      model_logp(*init_population))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "IXng90V0iXUr"
      },
      "outputs": [],
      "source": [
        "tfp.experimental.mcmc.sample_sequential_monte_carlo.__globals__['PRINT_DEBUG'] = False\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def run_smc():\n",
        "  n_stage, final_state, final_kernel_results = sample_sequential_monte_carlo_chain(\n",
        "    prior_log_prob_fn,\n",
        "    likelihood_log_prob_fn,\n",
        "    init_population,\n",
        "    make_kernel_fn=make_rwmh_kernel_fn,\n",
        "    max_num_steps=50\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "7bcAE4shblbJ"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "n_stage, final_state, final_kernel_results = run_smc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "GuNyYE_AHHUM"
      },
      "outputs": [],
      "source": [
        "var_name = ['b0', 'b1', 'mu_out', 'sigma_out', 'weight']\n",
        "posterior = {k:bij.forward(v).numpy()[np.newaxis, ...]\n",
        "             for k, v, bij in zip(var_name, final_state, unconstraining_bijectors)}\n",
        "\n",
        "az_trace = az.from_dict(posterior=posterior)\n",
        "az.plot_trace(az_trace);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eJdStyqpWmFD"
      },
      "source": [
        "### Comparison with NUTS\n",
        "Using SMC result as initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uiYUSMdnWglo"
      },
      "outputs": [],
      "source": [
        "nchain = 10\n",
        "b0, b1, mu_out, sigma_out, weight, _ = mdl_mixture.sample(nchain)\n",
        "init_state = [tf.ones_like(b0) * .1, tf.ones_like(b1),\n",
        "              tf.ones_like(mu_out), tf.ones_like(sigma_out),\n",
        "              tf.ones_like(weight) * .1]\n",
        "\n",
        "target_log_prob_fn = lambda *init_state: mdl_mixture.log_prob(\n",
        "    *init_state, dfhoggs['y'].values)\n",
        "\n",
        "step_size = [tf.math.reduce_std(x) for x in final_state]\n",
        "\n",
        "# target_log_prob_fn(*init_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "PtYZM3PyjJu8"
      },
      "outputs": [],
      "source": [
        "# @title A common `run_chain` function\n",
        "@tf.function(autograph=False, experimental_compile=True)\n",
        "def run_chain(init_state, step_size, target_log_prob_fn, unconstraining_bijectors,\n",
        "              num_steps=500, burnin=50):\n",
        "\n",
        "    def trace_fn(_, pkr):\n",
        "        return (\n",
        "            pkr.inner_results.inner_results.target_log_prob,\n",
        "            pkr.inner_results.inner_results.leapfrogs_taken,\n",
        "            pkr.inner_results.inner_results.has_divergence,\n",
        "            pkr.inner_results.inner_results.energy,\n",
        "            pkr.inner_results.inner_results.log_accept_ratio\n",
        "        )\n",
        "\n",
        "    kernel = tfp.mcmc.TransformedTransitionKernel(\n",
        "        inner_kernel=tfp.mcmc.NoUTurnSampler(\n",
        "            target_log_prob_fn,\n",
        "            step_size=step_size),\n",
        "        bijector=unconstraining_bijectors)\n",
        "\n",
        "    hmc = tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
        "        inner_kernel=kernel,\n",
        "        num_adaptation_steps=burnin,\n",
        "        step_size_setter_fn=lambda pkr, new_step_size: pkr._replace(\n",
        "            inner_results=pkr.inner_results._replace(step_size=new_step_size)),\n",
        "        step_size_getter_fn=lambda pkr: pkr.inner_results.step_size,\n",
        "        log_accept_prob_getter_fn=lambda pkr: pkr.inner_results.log_accept_ratio\n",
        "    )\n",
        "\n",
        "    # Sampling from the chain.\n",
        "    chain_state, sampler_stat = tfp.mcmc.sample_chain(\n",
        "        num_results=num_steps,\n",
        "        num_burnin_steps=burnin,\n",
        "        current_state=init_state,\n",
        "        kernel=hmc,\n",
        "        trace_fn=trace_fn)\n",
        "    return chain_state, sampler_stat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qiIYNm0XjJu-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "samples, sampler_stat = run_chain(\n",
        "    init_state, step_size, \n",
        "    target_log_prob_fn, unconstraining_bijectors, burnin=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "K3rqWtCgcr5m"
      },
      "outputs": [],
      "source": [
        "# Compare to a bad initialization with `step_size=1.`\n",
        "%%time\n",
        "\n",
        "_ = run_chain(\n",
        "    init_state, tf.cast(1., dtype=dtype), \n",
        "    target_log_prob_fn, unconstraining_bijectors, burnin=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eVK_-HqMjJvB"
      },
      "outputs": [],
      "source": [
        "# using the pymc3 naming convention\n",
        "sample_stats_name = ['lp', 'tree_size', 'diverging', 'energy', 'mean_tree_accept']\n",
        "sample_stats = {k:v.numpy().T for k, v in zip(sample_stats_name, sampler_stat)}\n",
        "\n",
        "var_name = ['b0', 'b1', 'mu_out', 'sigma_out', 'weight']\n",
        "posterior = {k:np.swapaxes(v.numpy(), 1, 0) \n",
        "             for k, v in zip(var_name, samples)}\n",
        "\n",
        "az_trace = az.from_dict(posterior=posterior, sample_stats=sample_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Wazp14vjIQAi"
      },
      "outputs": [],
      "source": [
        "az.plot_trace(az_trace, combined=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EsWYVKxfXG6x"
      },
      "outputs": [],
      "source": [
        "az.summary(az_trace)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xKVn5uilhys2"
      },
      "outputs": [],
      "source": [
        "np.sum(az_trace.sample_stats['diverging'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YN3wwSMWW7fh"
      },
      "source": [
        "### Also works with `TransformedTransitionKernel` and HMC as inner kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XZ0EfJxuUcr-"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.experimental.mcmc.sample_sequential_monte_carlo import gen_make_transform_hmc_kernel_fn\n",
        "\n",
        "OPTIMAL_HMC_ACCEPT = 0.651  # theoretical optimal acceptance rate for HMC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "c-ynhzd4UD_w"
      },
      "outputs": [],
      "source": [
        "_make_transform_hmc_kernel_fn = gen_make_transform_hmc_kernel_fn(\n",
        "    unconstraining_bijectors,\n",
        "    num_leapfrog_steps=10)\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def run_smc():\n",
        "  n_stage, final_state, final_kernel_results = sample_sequential_monte_carlo_chain(\n",
        "    prior_log_prob_fn_,\n",
        "    likelihood_log_prob_fn_,\n",
        "    prior_jd.sample(5000),\n",
        "    make_kernel_fn=_make_transform_hmc_kernel_fn,\n",
        "    optimal_accept=OPTIMAL_HMC_ACCEPT,\n",
        "    max_num_steps=50\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results\n",
        "\n",
        "n_stage, final_state, final_kernel_results = run_smc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hevwDSvDUEAK"
      },
      "outputs": [],
      "source": [
        "var_name = ['b0', 'b1', 'mu_out', 'sigma_out', 'weight']\n",
        "posterior = {k:v.numpy()[np.newaxis, ...]\n",
        "             for k, v in zip(var_name, final_state)}\n",
        "\n",
        "az_trace = az.from_dict(posterior=posterior)\n",
        "az.plot_trace(az_trace);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "Fo_hvSl1jJvK"
      },
      "outputs": [],
      "source": [
        "#@title Display fitted result and outliner probability\n",
        "b0, b1, mu_out, sigma_out, weight = [\n",
        "    tf.reshape(x, (500*10, 1)) for x in final_state]\n",
        "X = dfhoggs['x'].values[tf.newaxis, ...]\n",
        "sigma = dfhoggs['sigma_y'].values[tf.newaxis, ...]\n",
        "y_obs = dfhoggs['y'].values[tf.newaxis, ...]\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
        "x_fit = np.linspace(-1.1, 1.1, 100)[None, :]\n",
        "ax.plot(x_fit.T, np.transpose(b0[:10] + b1[:10]*x_fit), \n",
        "        alpha=.25, lw=1.5, color=dot_color)\n",
        "\n",
        "outlier_logp = tf.math.log(weight) + tfd.Normal(\n",
        "    loc=mu_out, scale=sigma+sigma_out).log_prob(y_obs)\n",
        "\n",
        "marg_logp = tfd.Mixture(\n",
        "    tfd.Categorical(probs=tf.stack(\n",
        "        [tf.repeat(1-weight, 20, axis=1),\n",
        "         tf.repeat(weight, 20, axis=1)], 2)),\n",
        "    [\n",
        "        tfd.Normal(loc=b0 + b1*X, scale=sigma),\n",
        "        tfd.Normal(loc=mu_out, scale=sigma+sigma_out)\n",
        "    ]\n",
        ").log_prob(y_obs)\n",
        "\n",
        "logp_outlier = outlier_logp - marg_logp\n",
        "\n",
        "ax.errorbar(dfhoggs['x'],\n",
        "            dfhoggs['y'], \n",
        "            yerr=dfhoggs['sigma_y'], \n",
        "            fmt=','+dot_color, \n",
        "            zorder=-1)\n",
        "\n",
        "p_outlier = np.exp(np.median(logp_outlier, axis=0))\n",
        "\n",
        "plt.scatter(dfhoggs['x'], dfhoggs['y'], c=p_outlier, cmap='plasma')\n",
        "plt.colorbar(ax=ax, label=\"outlier probability\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "50aOwxwfdPHx"
      },
      "source": [
        "## ODE (Lotka-Volterra equation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "gLYi-v9idQzn"
      },
      "outputs": [],
      "source": [
        "# Definition of parameters\n",
        "a = 1.\n",
        "b = 0.1\n",
        "c = 1.5\n",
        "d = 0.75\n",
        "\n",
        "# initial population of rabbits and foxes\n",
        "X0 = [10., 5.]\n",
        "# size of data\n",
        "size = 100\n",
        "# time lapse\n",
        "time = 15\n",
        "t = np.linspace(0, time, size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "01vnOSYNAIg6"
      },
      "outputs": [],
      "source": [
        "Run_quick_profiling = True #@param {type:\"boolean\"}\n",
        "if Run_quick_profiling:\n",
        "  from scipy.integrate import odeint\n",
        "  ode_dp = tfp.math.ode.DormandPrince()\n",
        "  ode_bdf = tfp.math.ode.BDF()\n",
        "\n",
        "  # Lotka - Volterra equation\n",
        "  def ode_fn(t, X, a, b, c, d):\n",
        "    \"\"\" Return the growth rate of fox and rabbit populations. \"\"\"\n",
        "    return tf.stack([a*X[0] - b*X[0]*X[1],\n",
        "                  -c*X[1] + d*b*X[0]*X[1]])\n",
        "\n",
        "  def run_scipy(init_cond, a, b, c, d):\n",
        "    ode_fn_ = lambda X, t: ode_fn(t, X, a, b, c, d)\n",
        "    return odeint(\n",
        "        ode_fn_,\n",
        "        y0=init_cond,\n",
        "        t=t,\n",
        "        rtol=0.1)\n",
        "\n",
        "  @tf.function(experimental_compile=True)\n",
        "  def run_dp(init_cond, a, b, c, d):\n",
        "    ode_fn_ = lambda t, X: ode_fn(t, X, a, b, c, d)\n",
        "    return ode_dp.solve(\n",
        "        ode_fn_, \n",
        "        initial_time=0., \n",
        "        initial_state=init_cond, \n",
        "        solution_times=t).states\n",
        "\n",
        "  @tf.function(experimental_compile=True)\n",
        "  def run_bdf(init_cond, a, b, c, d):\n",
        "    ode_fn_ = lambda t, X: ode_fn(t, X, a, b, c, d)\n",
        "    return ode_bdf.solve(\n",
        "        ode_fn_, \n",
        "        initial_time=0., \n",
        "        initial_state=init_cond, \n",
        "        solution_times=t).states\n",
        "\n",
        "  # run once\n",
        "  _ = run_dp(tf.cast(X0, tf.float32), a, b, c, d)\n",
        "  _ = run_bdf(tf.cast(X0, tf.float32), a, b, c, d)\n",
        "  _ = run_scipy(X0, a, b, c, d)\n",
        "\n",
        "  print('Scaler version')\n",
        "  print('DormandPrince')\n",
        "  %timeit _ = run_dp(tf.cast(X0, tf.float32), a, b, c, d)\n",
        "  print('BDF')\n",
        "  %timeit _ = run_bdf(tf.cast(X0, tf.float32), a, b, c, d)\n",
        "  print('Scipy')\n",
        "  %timeit _ = run_scipy(X0, a, b, c, d)\n",
        "\n",
        "  print('Batch version')\n",
        "  print('DormandPrince')\n",
        "  %timeit _ = run_dp(tf.cast(np.repeat([X0], 10, axis=0).T, tf.float32), *[tf.ones(10) * x for x in [a, b, c, d]])\n",
        "  print('BDF')\n",
        "  %timeit _ = run_bdf(tf.cast(np.repeat([X0], 10, axis=0).T, tf.float32), *[tf.ones(10) * x for x in [a, b, c, d]])\n",
        "  print('Scipy')\n",
        "  %timeit _ = np.asarray([run_scipy(X0, a_, b_, c_, d_) for a_, b_, c_, d_ in zip(*[tf.ones(10) * x for x in [a, b, c, d]])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "NqXKGHggupaB"
      },
      "outputs": [],
      "source": [
        "# ode_int = tfp.math.ode.BDF()\n",
        "ode_int = tfp.math.ode.DormandPrince()\n",
        "\n",
        "# simulator function\n",
        "@tf.function(experimental_compile=True)\n",
        "def competition_model(x0, y0, alpha, beta, gamma, delta):\n",
        "  # Lotka - Volterra equation\n",
        "  def ode_fn(t, X):\n",
        "    \"\"\" Return the growth rate of fox and rabbit populations. \"\"\"\n",
        "    dX_dt = alpha*X[0] - beta*X[0]*X[1]\n",
        "    dY_dt = -gamma*X[1] + delta*beta*X[0]*X[1]\n",
        "    return [dX_dt, dY_dt]\n",
        "\n",
        "  return ode_int.solve(\n",
        "      ode_fn,\n",
        "      initial_time=0.,\n",
        "      initial_state=[x0, y0],\n",
        "      solution_times=t)\n",
        "\n",
        "# Generating noisy data to be used as observed data.\n",
        "def add_noise(x0, y0, a, b, c, d):\n",
        "  noise = np.random.normal(size=(size, 2))\n",
        "  simulated = tf.stack(competition_model(\n",
        "      x0, y0, a, b, c, d).states).numpy().T\n",
        "  simulated += noise\n",
        "  indexes = np.sort(np.random.randint(low=0, high=size, size=size))    \n",
        "  return simulated[indexes]\n",
        "\n",
        "observed = add_noise(X0[0], X0[1], a, b, c, d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_fSbghVJu4LU"
      },
      "outputs": [],
      "source": [
        "# plotting observed data.\n",
        "def plot_ode(state0, state1):\n",
        "  _, ax = plt.subplots(figsize=(12,4))\n",
        "  ax.plot(state0, 'o', label='prey')\n",
        "  ax.plot(state1, 'x', label='predator')\n",
        "  ax.set_xlabel('time')\n",
        "  ax.set_ylabel('population')\n",
        "  ax.set_title('Observed data')\n",
        "  return ax\n",
        "\n",
        "ax = plot_ode(observed[:,0], observed[:,1])\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9cPR7F9UmLz7"
      },
      "outputs": [],
      "source": [
        "ode_prior = tfd.JointDistributionSequential([\n",
        "  tfd.Normal(X0[0], .25),\n",
        "  tfd.Normal(X0[1], .25),\n",
        "  tfd.Normal(1., .5),\n",
        "  tfd.Normal(.5, .1),\n",
        "  tfd.Normal(2., .5),\n",
        "  tfd.Normal(1., .1),\n",
        "])\n",
        "\n",
        "result = competition_model(*ode_prior.sample(5))\n",
        "ax = plot_ode(result.states[0], result.states[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "E5R6MuDG2JB2"
      },
      "outputs": [],
      "source": [
        "epsilon = .5\n",
        "kernel = tfd.Normal(0., epsilon)\n",
        "\n",
        "def absolute_error(observed, simulated):\n",
        "  return tf.reduce_mean(\n",
        "      tf.abs(observed-simulated), axis=[0, 1])\n",
        "\n",
        "def pseudolikelihood(x0, y0, alpha, beta, gamma, delta):\n",
        "  sim_data = competition_model(x0, y0, alpha, beta, gamma, delta)\n",
        "  distance = absolute_error(\n",
        "      observed.T[..., tf.newaxis], tf.stack(sim_data.states))\n",
        "  return kernel.log_prob(distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u3e5qO8c-SvK"
      },
      "source": [
        "FYI: ODE with correct gradient on parameter inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "MUtxDaHZCRoD"
      },
      "outputs": [],
      "source": [
        "variables = ode_prior.sample(5)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(variables)\n",
        "  log_p = pseudolikelihood(*variables)\n",
        "grad = tape.gradient(log_p, variables)\n",
        "print(grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9-cMqfKgDeSs"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def competition_model2(x0, y0, alpha, beta, gamma, delta):\n",
        "  # Lotka - Volterra equation\n",
        "  def ode_fn(t, X, alpha, beta, gamma, delta):\n",
        "    \"\"\" Return the growth rate of fox and rabbit populations. \"\"\"\n",
        "    dX_dt = alpha*X[0] - beta*X[0]*X[1]\n",
        "    dY_dt = -gamma*X[1] + delta*beta*X[0]*X[1]\n",
        "    return [dX_dt, dY_dt]\n",
        "\n",
        "  return ode_int.solve(\n",
        "      ode_fn,\n",
        "      initial_time=0.,\n",
        "      initial_state=[x0, y0],\n",
        "      solution_times=t,\n",
        "      constants={'alpha': alpha, \n",
        "                 'beta': beta,\n",
        "                 'gamma': gamma,\n",
        "                 'delta': delta})\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def pseudolikelihood2(x0, y0, alpha, beta, gamma, delta):\n",
        "  sim_data = competition_model2(x0, y0, alpha, beta, gamma, delta)\n",
        "  distance = absolute_error(\n",
        "      observed.T[..., tf.newaxis], tf.stack(sim_data.states[:2]))\n",
        "  return kernel.log_prob(distance)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(variables)\n",
        "  log_p = pseudolikelihood2(*variables)\n",
        "grad = tape.gradient(log_p, variables)\n",
        "print(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7J0oYwMZ9Fux"
      },
      "source": [
        "Currently, the Random Walk kernel is adapting to the scale of the posterior samples at each inverse temperature, which could give proposal that are difficult to simulated from (i.e., the TFP ode solver runs too slows with some inputs that produce large ode states)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "EDU3-XmPVb0U"
      },
      "outputs": [],
      "source": [
        "def _make_rescale_rwmh_kernel_fn(target_log_prob_fn, init_state, scalings, seed=None):\n",
        "  state_std = [tf.math.reduce_std(x, axis=0, keepdims=True) for x in init_state]\n",
        "  step_size = [s * tf.cast(\n",
        "      ps.reshape(scalings * .1,  # \u003c== additional rescaling here\n",
        "                 ps.pad(ps.shape(scalings),\n",
        "                        paddings=[[0, ps.rank(s)-1]],\n",
        "                        constant_values=1)\n",
        "                 ), s.dtype)for s in state_std]\n",
        "  return tfp.mcmc.RandomWalkMetropolis(\n",
        "      target_log_prob_fn,\n",
        "      new_state_fn=tfp.mcmc.random_walk_normal_fn(scale=step_size),\n",
        "      seed=seed)\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def run_smc():\n",
        "  n_stage, final_state, final_kernel_results = sample_sequential_monte_carlo_chain(\n",
        "    ode_prior.log_prob,\n",
        "    pseudolikelihood,\n",
        "    ode_prior.sample(1000),\n",
        "    make_kernel_fn=_make_rescale_rwmh_kernel_fn,\n",
        "    optimal_accept=.35,\n",
        "    max_num_steps=50,\n",
        "    # print_debug=True\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results\n",
        "\n",
        "n_stage, final_state, final_kernel_results = run_smc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_uyrG6DNEoHZ"
      },
      "outputs": [],
      "source": [
        "var_name = ['x0', 'y0', 'alpha', 'beta', 'gamma', 'delta']\n",
        "posterior = {k:v.numpy()[np.newaxis, ...]\n",
        "             for k, v in zip(var_name, final_state)}\n",
        "\n",
        "az_trace = az.from_dict(posterior=posterior)\n",
        "az.plot_trace(az_trace);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oj6orSczDLiJ"
      },
      "outputs": [],
      "source": [
        "posterior_predictive = tf.stack(competition_model(*final_state).states)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "_kThXfRFDDIv"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(14,6))\n",
        "ax.plot(observed[:, 0], 'o', label='prey', c='C0')\n",
        "ax.plot(observed[:, 1], 'x', label='predator', c='C1')\n",
        "ax.plot(tf.transpose(tf.reduce_mean(posterior_predictive, axis=-1)), linewidth=2.5)\n",
        "for i in np.random.randint(0, 50, 1000):\n",
        "    ax.plot(posterior_predictive[0, :, i], alpha=0.05, c='C2', zorder=0)\n",
        "    ax.plot(posterior_predictive[1, :, i], alpha=0.05, c='C3', zorder=0)\n",
        "ax.set_xlabel('time')\n",
        "ax.set_ylabel('population')\n",
        "ax.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YwMRGo_1dRQX"
      },
      "source": [
        "## Bayesian State Space Model (BSTS)\n",
        "\n",
        "https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Structural_Time_Series_Modeling_Case_Studies_Atmospheric_CO2_and_Electricity_Demand.ipynb\n",
        "\n",
        "Update posterior samples conditioned on the new observation.\n",
        "\n",
        "Here we sampled from the posterior conditioned on the training data set, update the posterior using the test data set, and compare the result with running the inference on the whole data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "FaVBsNdnwq_M"
      },
      "outputs": [],
      "source": [
        "import matplotlib.dates as mdates\n",
        "from pandas.plotting import register_matplotlib_converters\n",
        "register_matplotlib_converters()\n",
        "\n",
        "# CO2 readings from Mauna Loa observatory, monthly beginning January 1966\n",
        "# Original source: http://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record\n",
        "co2_by_month = np.array('320.62,321.60,322.39,323.70,324.08,323.75,322.38,320.36,318.64,318.10,319.78,321.03,322.33,322.50,323.04,324.42,325.00,324.09,322.54,320.92,319.25,319.39,320.73,321.96,322.57,323.15,323.89,325.02,325.57,325.36,324.14,322.11,320.33,320.25,321.32,322.89,324.00,324.42,325.63,326.66,327.38,326.71,325.88,323.66,322.38,321.78,322.85,324.12,325.06,325.98,326.93,328.14,328.08,327.67,326.34,324.69,323.10,323.06,324.01,325.13,326.17,326.68,327.17,327.79,328.92,328.57,327.36,325.43,323.36,323.56,324.80,326.01,326.77,327.63,327.75,329.73,330.07,329.09,328.04,326.32,324.84,325.20,326.50,327.55,328.55,329.56,330.30,331.50,332.48,332.07,330.87,329.31,327.51,327.18,328.16,328.64,329.35,330.71,331.48,332.65,333.09,332.25,331.18,329.39,327.43,327.37,328.46,329.57,330.40,331.40,332.04,333.31,333.97,333.60,331.90,330.06,328.56,328.34,329.49,330.76,331.75,332.56,333.50,334.58,334.88,334.33,333.05,330.94,329.30,328.94,330.31,331.68,332.93,333.42,334.70,336.07,336.75,336.27,334.92,332.75,331.59,331.16,332.40,333.85,334.97,335.38,336.64,337.76,338.01,337.89,336.54,334.68,332.76,332.55,333.92,334.95,336.23,336.76,337.96,338.88,339.47,339.29,337.73,336.09,333.92,333.86,335.29,336.73,338.01,338.36,340.07,340.77,341.47,341.17,339.56,337.60,335.88,336.02,337.10,338.21,339.24,340.48,341.38,342.51,342.91,342.25,340.49,338.43,336.69,336.86,338.36,339.61,340.75,341.61,342.70,343.57,344.14,343.35,342.06,339.81,337.98,337.86,339.26,340.49,341.38,342.52,343.10,344.94,345.76,345.32,343.98,342.38,339.87,339.99,341.15,342.99,343.70,344.50,345.28,347.06,347.43,346.80,345.39,343.28,341.07,341.35,342.98,344.22,344.97,345.99,347.42,348.35,348.93,348.25,346.56,344.67,343.09,342.80,344.24,345.56,346.30,346.95,347.85,349.55,350.21,349.55,347.94,345.90,344.85,344.17,345.66,346.90,348.02,348.48,349.42,350.99,351.85,351.26,349.51,348.10,346.45,346.36,347.81,348.96,350.43,351.73,352.22,353.59,354.22,353.79,352.38,350.43,348.73,348.88,350.07,351.34,352.76,353.07,353.68,355.42,355.67,355.12,353.90,351.67,349.80,349.99,351.30,352.52,353.66,354.70,355.38,356.20,357.16,356.23,354.81,352.91,350.96,351.18,352.83,354.21,354.72,355.75,357.16,358.60,359.34,358.24,356.17,354.02,352.15,352.21,353.75,354.99,355.99,356.72,357.81,359.15,359.66,359.25,357.02,355.00,353.01,353.31,354.16,355.40,356.70,357.17,358.38,359.46,360.28,359.60,357.57,355.52,353.69,353.99,355.34,356.80,358.37,358.91,359.97,361.26,361.69,360.94,359.55,357.48,355.84,356.00,357.58,359.04,359.97,361.00,361.64,363.45,363.80,363.26,361.89,359.45,358.05,357.75,359.56,360.70,362.05,363.24,364.02,364.71,365.41,364.97,363.65,361.48,359.45,359.61,360.76,362.33,363.18,363.99,364.56,366.36,366.80,365.63,364.47,362.50,360.19,360.78,362.43,364.28,365.33,366.15,367.31,368.61,369.30,368.88,367.64,365.78,363.90,364.23,365.46,366.97,368.15,368.87,369.59,371.14,371.00,370.35,369.27,366.93,364.64,365.13,366.68,368.00,369.14,369.46,370.51,371.66,371.83,371.69,370.12,368.12,366.62,366.73,368.29,369.53,370.28,371.50,372.12,372.86,374.02,373.31,371.62,369.55,367.96,368.09,369.68,371.24,372.44,373.08,373.52,374.85,375.55,375.40,374.02,371.48,370.70,370.25,372.08,373.78,374.68,375.62,376.11,377.65,378.35,378.13,376.61,374.48,372.98,373.00,374.35,375.69,376.79,377.36,378.39,380.50,380.62,379.55,377.76,375.83,374.05,374.22,375.84,377.44,378.34,379.61,380.08,382.05,382.24,382.08,380.67,378.67,376.42,376.80,378.31,379.96,381.37,382.02,382.56,384.37,384.92,384.03,382.28,380.48,378.81,379.06,380.14,381.66,382.58,383.71,384.34,386.23,386.41,385.87,384.45,381.84,380.86,380.86,382.36,383.61,385.07,385.84,385.83,386.77,388.51,388.05,386.25,384.08,383.09,382.78,384.01,385.11,386.65,387.12,388.52,389.57,390.16,389.62,388.07,386.08,384.65,384.33,386.05,387.49,388.55,390.07,391.01,392.38,393.22,392.24,390.33,388.52,386.84,387.16,388.67,389.81,391.30,391.92,392.45,393.37,394.28,393.69,392.59,390.21,389.00,388.93,390.24,391.80,393.07,393.35,394.36,396.43,396.87,395.88,394.52,392.54,391.13,391.01,392.95,394.34,395.61,396.85,397.26,398.35,399.98,398.87,397.37,395.41,393.39,393.70,395.19,396.82,397.92,398.10,399.47,401.33,401.88,401.31,399.07,397.21,395.40,395.65,397.23,398.79,399.85,400.31,401.51,403.45,404.10,402.88,401.61,399.00,397.50,398.28,400.24,401.89,402.65,404.16,404.85,407.57,407.66,407.00,404.50,402.24,401.01,401.50,403.64,404.55,406.07,406.64,407.06,408.95,409.91,409.12,407.20,405.24,403.27,403.64,405.17,406.75,408.05,408.34,409.25,410.30,411.30,410.88,408.90,407.10,405.59,405.99,408.12,409.23,410.92'.split(',')).astype(np.float32)\n",
        "\n",
        "co2_by_month = co2_by_month\n",
        "num_forecast_steps = 12 * 10 # Forecast the final ten years, given previous data\n",
        "co2_by_month_training_data = co2_by_month[:-num_forecast_steps]\n",
        "co2_by_month_validation_data = co2_by_month[-num_forecast_steps:-int(num_forecast_steps/2)]\n",
        "co2_by_month_training_data2 = co2_by_month[:-int(num_forecast_steps/2)]\n",
        "co2_by_month_test_data = co2_by_month[-int(num_forecast_steps/2):]\n",
        "\n",
        "co2_dates = np.arange(\"1966-01\", \"2019-02\", dtype=\"datetime64[M]\")\n",
        "co2_loc = mdates.YearLocator(3)\n",
        "co2_fmt = mdates.DateFormatter('%Y')\n",
        "\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.plot(co2_dates[:-num_forecast_steps], co2_by_month_training_data, lw=2, label=\"training data\")\n",
        "ax.xaxis.set_major_locator(co2_loc)\n",
        "ax.xaxis.set_major_formatter(co2_fmt)\n",
        "ax.set_ylabel(\"Atmospheric CO2 concentration (ppm)\")\n",
        "ax.set_xlabel(\"Year\")\n",
        "fig.suptitle(\"Monthly average CO2 concentration, Mauna Loa, Hawaii\",\n",
        "             fontsize=15)\n",
        "ax.text(0.99, .02,\n",
        "        \"Source: Scripps Institute for Oceanography CO2 program\\nhttp://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record\",\n",
        "        transform=ax.transAxes,\n",
        "        horizontalalignment=\"right\",\n",
        "        alpha=0.5)\n",
        "fig.autofmt_xdate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "PEFsJToWxgN_"
      },
      "outputs": [],
      "source": [
        "def build_model(observed_time_series):\n",
        "  trend = tfp.sts.LocalLinearTrend(observed_time_series=observed_time_series)\n",
        "  seasonal = tfp.sts.Seasonal(\n",
        "      num_seasons=12, observed_time_series=observed_time_series)\n",
        "  model = tfp.sts.Sum([trend, seasonal], observed_time_series=observed_time_series)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ik4tuoAawUrO"
      },
      "outputs": [],
      "source": [
        "co2_model = build_model(co2_by_month_training_data)\n",
        "\n",
        "# Build the variational surrogate posteriors `qs`.\n",
        "variational_posteriors = tfp.sts.build_factored_surrogate_posterior(\n",
        "    model=co2_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "lkhuF9GFoOH4"
      },
      "outputs": [],
      "source": [
        "#@title Minimize the variational loss.\n",
        "\n",
        "# Allow external control of optimization to reduce test runtimes.\n",
        "num_variational_steps = 150 # @param { isTemplate: true}\n",
        "num_variational_steps = int(num_variational_steps)\n",
        "\n",
        "optimizer = tf.optimizers.Adam(learning_rate=.1)\n",
        "# Using fit_surrogate_posterior to build and optimize the variational loss function.\n",
        "@tf.function(experimental_compile=True)\n",
        "def train():\n",
        "  elbo_loss_curve = tfp.vi.fit_surrogate_posterior(\n",
        "    target_log_prob_fn=co2_model.joint_log_prob(\n",
        "        observed_time_series=co2_by_month_training_data),\n",
        "    surrogate_posterior=variational_posteriors,\n",
        "    optimizer=optimizer,\n",
        "    num_steps=num_variational_steps)\n",
        "  return elbo_loss_curve\n",
        "\n",
        "elbo_loss_curve = train()\n",
        "\n",
        "plt.plot(elbo_loss_curve)\n",
        "plt.show()\n",
        "\n",
        "# Draw samples from the variational posterior.\n",
        "q_samples_co2_ = variational_posteriors.sample(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dfHakFJ-zLm8"
      },
      "outputs": [],
      "source": [
        "print(\"Inferred parameters:\")\n",
        "for param in co2_model.parameters:\n",
        "  print(\"{}: {} +- {}\".format(param.name,\n",
        "                              np.mean(q_samples_co2_[param.name], axis=0),\n",
        "                              np.std(q_samples_co2_[param.name], axis=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "form",
        "colab": {},
        "colab_type": "code",
        "id": "s6uPGV4sRHz_"
      },
      "outputs": [],
      "source": [
        "#@title MCMC (very slow)\n",
        "%%time\n",
        "Run_MCMC = False #@param {type:\"boolean\"}\n",
        "if Run_MCMC:\n",
        "  posterior_sample = variational_posteriors.sample(10)\n",
        "  initial_state = [posterior_sample[p.name] for p in co2_model.parameters]\n",
        "\n",
        "  q_dists_by_name, _ = variational_posteriors.sample_distributions()\n",
        "  initial_step_size = [\n",
        "    q_dists_by_name[p.name].distribution.stddev()\n",
        "    for p in co2_model.parameters]\n",
        "\n",
        "  num_results = 100\n",
        "  num_warmup_steps = 50\n",
        "  num_leapfrog_steps = 15\n",
        "\n",
        "  observed_time_series = sts_util.pad_batch_dimension_for_multiple_chains(\n",
        "      co2_by_month_training_data, co2_model, chain_batch_shape=10)\n",
        "  target_log_prob_fn = co2_model.joint_log_prob(observed_time_series)\n",
        "\n",
        "  # Run HMC to sample from the posterior on parameters.\n",
        "  @tf.function(\n",
        "      input_signature=[tf.TensorSpec([], dtype=tf.int32),\n",
        "                       tf.TensorSpec([], dtype=tf.int32)],\n",
        "      autograph=False,\n",
        "      experimental_compile=True)\n",
        "  def run_hmc(num_results=100, num_warmup_steps=50):\n",
        "    return tfp.mcmc.sample_chain(\n",
        "        num_results=num_results,\n",
        "        current_state=initial_state,\n",
        "        num_burnin_steps=num_warmup_steps,\n",
        "        kernel=tfp.mcmc.DualAveragingStepSizeAdaptation(\n",
        "            inner_kernel=tfp.mcmc.TransformedTransitionKernel(\n",
        "                inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
        "                    target_log_prob_fn=target_log_prob_fn,\n",
        "                    step_size=initial_step_size,\n",
        "                    num_leapfrog_steps=num_leapfrog_steps,\n",
        "                    state_gradients_are_stopped=True),\n",
        "                bijector=[param.bijector for param in co2_model.parameters]),\n",
        "            num_adaptation_steps=num_warmup_steps),\n",
        "        trace_fn=None\n",
        "        )\n",
        "\n",
        "  _ = run_hmc(1, 1)\n",
        "  print('XLA compile done.')\n",
        "  samples = run_hmc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xFrhy5hB11n5"
      },
      "source": [
        "### Conditioned on validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "eB8vq-4WXTbH"
      },
      "outputs": [],
      "source": [
        "samples = variational_posteriors.sample(10)\n",
        "new_conditional_logp = co2_model.joint_log_prob(co2_by_month_validation_data)\n",
        "\n",
        "new_conditional_logp(**samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CzEb496MdNm8"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.internal import distribution_util\n",
        "from tensorflow_probability.python.sts.internal import util as sts_util\n",
        "\n",
        "def prior_log_prob_fn(*param_vals):\n",
        "  return sum([\n",
        "      param.prior.log_prob(param_val)\n",
        "      for (param, param_val) in zip(co2_model.parameters, \n",
        "                                    param_vals)\n",
        "  ])\n",
        "\n",
        "[\n",
        "    observed_time_series_,\n",
        "    mask\n",
        "] = sts_util.canonicalize_observed_time_series_with_mask(\n",
        "    co2_by_month_validation_data)\n",
        "\n",
        "num_timesteps = distribution_util.prefer_static_value(\n",
        "    tf.shape(observed_time_series_))[-2]\n",
        "\n",
        "def likelihood_log_prob_fn(*param_vals):\n",
        "  # Build a linear Gaussian state space model and evaluate the marginal\n",
        "  # log_prob on observations.\n",
        "  lgssm = co2_model.make_state_space_model(\n",
        "      param_vals=param_vals, num_timesteps=num_timesteps)\n",
        "  observation_lp = lgssm.log_prob(observed_time_series_,\n",
        "                                  mask=mask)\n",
        "  return observation_lp\n",
        "\n",
        "likelihood_log_prob_fn(*samples.values()) + prior_log_prob_fn(*samples.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sZjrQGTEg7Ho"
      },
      "outputs": [],
      "source": [
        "samples = variational_posteriors.sample(1000)\n",
        "init_state = list(samples.values())\n",
        "\n",
        "_make_transform_hmc_kernel_fn = gen_make_transform_hmc_kernel_fn(\n",
        "    [param.bijector for param in co2_model.parameters],\n",
        "    num_leapfrog_steps=5)\n",
        "\n",
        "@tf.function(experimental_compile=True)\n",
        "def run_smc():\n",
        "  (\n",
        "      n_stage, final_state, final_kernel_results\n",
        "  ) = sample_sequential_monte_carlo_chain(\n",
        "    prior_log_prob_fn,\n",
        "    likelihood_log_prob_fn,\n",
        "    init_state,\n",
        "    make_kernel_fn=_make_transform_hmc_kernel_fn,\n",
        "    optimal_accept=OPTIMAL_HMC_ACCEPT,\n",
        "    max_num_steps=50\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results\n",
        "\n",
        "n_stage, final_state, final_kernel_results = run_smc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uyDF6wkAF9_h"
      },
      "source": [
        "Comparing with using yesterday's (Variational) posterior as today's prior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wVAHGzn4E24-"
      },
      "outputs": [],
      "source": [
        "@tf.function(experimental_compile=True)\n",
        "def run_smc_():\n",
        "  (\n",
        "      n_stage, final_state, final_kernel_results\n",
        "  ) = sample_sequential_monte_carlo_chain(\n",
        "    variational_posteriors.log_prob,\n",
        "    likelihood_log_prob_fn,\n",
        "    init_state,\n",
        "    make_kernel_fn=_make_transform_hmc_kernel_fn,\n",
        "    optimal_accept=OPTIMAL_HMC_ACCEPT,\n",
        "    max_num_steps=50\n",
        "    )\n",
        "  return n_stage, final_state, final_kernel_results\n",
        "\n",
        "n_stage, final_state_, final_kernel_results = run_smc_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "bk-o8PP1DV2I"
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(2, 2, figsize=(14, 6))\n",
        "ax = ax.flatten()\n",
        "for i, param in enumerate(co2_model.parameters):\n",
        "  ax[i].hist(init_state[i], bins=50,\n",
        "             density=True, histtype='step', lw=2, label='Previous Posterior (VI)')\n",
        "  ax[i].hist(final_state[i], bins=50,\n",
        "             density=True, histtype='step', lw=2, label='New Posterior (SMC)')\n",
        "  ax[i].hist(final_state_[i], bins=50,\n",
        "             density=True, histtype='step', lw=2, label='SMC-updated Posterior')\n",
        "  # ax[i].hist(param.prior.sample(1000), bins=50,\n",
        "  #            density=True, histtype='step', lw=2, label='Latent Truth')\n",
        "  ax[i].set_title(param.name)\n",
        "plt.legend();\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2OFu7RFJ8jl"
      },
      "source": [
        "### Forecasting conditioned on the newest observed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "FQSCGdHlNp3K"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "posterior_samples = collections.OrderedDict()\n",
        "for key, values in zip(\n",
        "    q_samples_co2_.keys(),\n",
        "    # init_state\n",
        "    # final_state\n",
        "    final_state_\n",
        "    ):\n",
        "  posterior_samples[key] = values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "r4ldbsNDza00"
      },
      "outputs": [],
      "source": [
        "co2_model_ = build_model(co2_by_month_validation_data)\n",
        "\n",
        "co2_forecast_dist = tfp.sts.forecast(\n",
        "    co2_model_,\n",
        "    observed_time_series=co2_by_month_validation_data,\n",
        "    parameter_samples=posterior_samples,\n",
        "    num_steps_forecast=num_forecast_steps // 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ULm_Z8Oe0Lhd"
      },
      "outputs": [],
      "source": [
        "num_samples = 20\n",
        "\n",
        "co2_forecast_mean, co2_forecast_scale, co2_forecast_samples = (\n",
        "    co2_forecast_dist.mean().numpy()[..., 0],\n",
        "    co2_forecast_dist.stddev().numpy()[..., 0],\n",
        "    co2_forecast_dist.sample(num_samples).numpy()[..., 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cvvdsx5JLCBM"
      },
      "outputs": [],
      "source": [
        "n1 = len(co2_by_month_validation_data)\n",
        "n2 = len(co2_by_month_test_data)\n",
        "plt.plot(np.arange(n1),\n",
        "         co2_by_month_validation_data,\n",
        "         label='New observation')\n",
        "plt.plot(np.arange(n1, n1+n2),\n",
        "         tf.transpose(co2_forecast_samples), \n",
        "         color='k',\n",
        "         alpha=.1)\n",
        "plt.plot(np.arange(n1, n1+n2),\n",
        "         co2_by_month_test_data,\n",
        "         color='r',\n",
        "         alpha=.5,\n",
        "         label='Actual observed on predicted dates')\n",
        "plt.plot(np.arange(n1, n1+n2),\n",
        "         co2_forecast_mean, \n",
        "         '--g',\n",
        "         label='Mean prediction')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "06mJ5f1g4FUV"
      },
      "source": [
        "# Follow ups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FUESNhbR4RjB"
      },
      "source": [
        "\n",
        "\n",
        "*   Other transitional kernel from https://arxiv.org/pdf/1903.04797.pdf\n",
        "*   Using similar amortization and temperature annealing for VI (e.g., Stein Variational Gradient Descent)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "81vLOVjCi7FQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "SMC Demo",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
